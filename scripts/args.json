{
    "model_args": {
      "model_name_or_path": "meta-llama/Llama-2-7b-hf"
    },
    "data_args": {
      "dataset_name": "yahma/alpaca-cleaned" 
    },
    "training_args": {
      "output_dir": "llama2_finetuned",
      "num_train_epochs": 1
    },
    "lora_args": {
      "r": 16,
      "lora_alpha": 64,
      "target_modules": "all-linear",
      "lora_dropout": 0.1,
      "bias": "none",
      "task_type": "CAUSAL_LM"
    }
  }